# 一、简介

互信息（Mutual Information, MI）是衡量两个随机变量之间相互依赖程度（相关性）的量，能够量化通过一个变量获取的关于另一个变量的信息量。
$$
I(x, z) = \int_z\int_xp(x, z)\log \frac{p(x, z)}{p(x)p(z)}dxdz
\\=KL(p(x,z)∥p(x)p(z))
$$

 互信息的性质

- **非负性**：互信息总是非负的，当且仅当两个变量独立时，互信息为零。

  > 变量独立时，对所有(x,z) ：$p(x, z) = p(x)p(z)$
  >
  > 不独立时，存在但不一定是所有(x, z)：$p(x, z) ≠ p(x)p(z)$

- **对称性**：互信息是对称的，即 $I(X;Y)=I(Y;X)$ 。

- **与熵的关系**：互信息与熵、条件熵等信息量有密切关系，能够反映变量之间的依赖性。



互信息非负性是由KL散度非负性而来，KL散度非负性的证明可以看纸质笔记，改天整理。



要让隐变量包含尽可能多的输入数据的信息，就是要让x和z有尽可能高的相关性，进而有尽可能大的互信息。

![image-20251206175034245](./assets/image-20251206175034245.png)

2个标准正态分布的联合概率分布图，左侧2个变量有正相关性，右侧2个变量独立。



# 二、参考

[从变分编码、信息瓶颈到正态分布：论遗忘的重要性 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/6181)

[最小熵原理（四）：“物以类聚”之从图书馆到词向量 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/6191)

[深度学习的互信息：无监督提取特征 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/6024)